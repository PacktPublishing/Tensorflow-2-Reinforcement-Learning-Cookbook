{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy gradient algorithm and agent with neural network policy\n",
    "Chapter 2, TensorFlow 2 Reinforcement Learning Cookbook | Praveen Palanisamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(keras.Model):\n",
    "    def __init__(self, action_dim=1):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = layers.Dense(24, activation=\"relu\")\n",
    "        self.fc2 = layers.Dense(36, activation=\"relu\")\n",
    "        self.fc3 = layers.Dense(action_dim, activation=\"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def process(self, observations):\n",
    "        # Process batch observations using `call(x)` behind-the-scenes\n",
    "        action_probabilities = self.predict_on_batch(observations)\n",
    "        return action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, action_dim=1):\n",
    "        \"\"\"Agent with a neural-network brain powered policy\n",
    "\n",
    "        Args:\n",
    "            action_dim (int): Action dimension\n",
    "        \"\"\"\n",
    "        self.policy_net = PolicyNet(action_dim=action_dim)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def policy(self, observation):\n",
    "        observation = observation.reshape(1, -1)\n",
    "        observation = tf.convert_to_tensor(observation, dtype=tf.float32)\n",
    "        action_logits = self.policy_net(observation)\n",
    "        action = tf.random.categorical(tf.math.log(action_logits), num_samples=1)\n",
    "        return action\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        action = self.policy(observation).numpy()\n",
    "        return action.squeeze()\n",
    "\n",
    "    def learn(self, states, rewards, actions):\n",
    "        discounted_reward = 0\n",
    "        discounted_rewards = []\n",
    "        rewards.reverse()\n",
    "        for r in rewards:\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            discounted_rewards.append(discounted_reward)\n",
    "        discounted_rewards.reverse()\n",
    "\n",
    "        for state, reward, action in zip(states, discounted_rewards, actions):\n",
    "            with tf.GradientTape() as tape:\n",
    "                action_probabilities = self.policy_net(np.array([state]), training=True)\n",
    "                loss = self.loss(action_probabilities, action, reward)\n",
    "            grads = tape.gradient(loss, self.policy_net.trainable_variables)\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(grads, self.policy_net.trainable_variables)\n",
    "            )\n",
    "\n",
    "    def loss(self, action_probabilities, action, reward):\n",
    "        dist = tfp.distributions.Categorical(\n",
    "            probs=action_probabilities, dtype=tf.float32\n",
    "        )\n",
    "        log_prob = dist.log_prob(action)\n",
    "        loss = -log_prob * reward\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent, env: gym.Env, episodes: int, render=True):\n",
    "    \"\"\"Train `agent` in `env` for `episodes`\n",
    "\n",
    "    Args:\n",
    "        agent (Agent): Agent to train\n",
    "        env (gym.Env): Environment to train the agent\n",
    "        episodes (int): Number of episodes to train\n",
    "        render (bool): True=Enable/False=Disable rendering; Default=True\n",
    "    \"\"\"\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                agent.learn(states, rewards, actions)\n",
    "                print(\"\\n\")\n",
    "            print(f\"Episode#:{episode} ep_reward:{total_reward}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#:0 ep_reward:-1.0\r",
      "Episode#:0 ep_reward:-2.0\r",
      "Episode#:0 ep_reward:-3.0\r",
      "Episode#:0 ep_reward:-4.0\r",
      "Episode#:0 ep_reward:-5.0\r",
      "Episode#:0 ep_reward:-6.0\r",
      "Episode#:0 ep_reward:-7.0\r",
      "Episode#:0 ep_reward:-8.0\r",
      "Episode#:0 ep_reward:-9.0\r",
      "Episode#:0 ep_reward:-10.0\r",
      "Episode#:0 ep_reward:-11.0\r",
      "Episode#:0 ep_reward:-12.0\r",
      "Episode#:0 ep_reward:-13.0\r",
      "Episode#:0 ep_reward:-14.0\r",
      "Episode#:0 ep_reward:-15.0\r",
      "Episode#:0 ep_reward:-16.0\r",
      "Episode#:0 ep_reward:-17.0\r",
      "Episode#:0 ep_reward:-18.0\r",
      "Episode#:0 ep_reward:-19.0\r",
      "Episode#:0 ep_reward:-20.0\r",
      "Episode#:0 ep_reward:-21.0\r",
      "Episode#:0 ep_reward:-22.0\r",
      "Episode#:0 ep_reward:-23.0\r",
      "Episode#:0 ep_reward:-24.0\r",
      "Episode#:0 ep_reward:-25.0\r",
      "Episode#:0 ep_reward:-26.0\r",
      "Episode#:0 ep_reward:-27.0\r",
      "Episode#:0 ep_reward:-28.0\r",
      "Episode#:0 ep_reward:-29.0\r",
      "Episode#:0 ep_reward:-30.0\r",
      "Episode#:0 ep_reward:-31.0\r",
      "Episode#:0 ep_reward:-32.0\r",
      "Episode#:0 ep_reward:-33.0\r",
      "Episode#:0 ep_reward:-34.0\r",
      "Episode#:0 ep_reward:-35.0\r",
      "Episode#:0 ep_reward:-36.0\r",
      "Episode#:0 ep_reward:-37.0\r",
      "Episode#:0 ep_reward:-38.0\r",
      "Episode#:0 ep_reward:-39.0\r",
      "Episode#:0 ep_reward:-40.0\r",
      "Episode#:0 ep_reward:-41.0\r",
      "Episode#:0 ep_reward:-42.0\r",
      "Episode#:0 ep_reward:-43.0\r",
      "Episode#:0 ep_reward:-44.0\r",
      "Episode#:0 ep_reward:-45.0\r",
      "Episode#:0 ep_reward:-46.0\r",
      "Episode#:0 ep_reward:-47.0\r",
      "Episode#:0 ep_reward:-48.0\r",
      "Episode#:0 ep_reward:-49.0\r",
      "Episode#:0 ep_reward:-50.0\r",
      "Episode#:0 ep_reward:-51.0\r",
      "Episode#:0 ep_reward:-52.0\r",
      "Episode#:0 ep_reward:-53.0\r",
      "Episode#:0 ep_reward:-54.0\r",
      "Episode#:0 ep_reward:-55.0\r",
      "Episode#:0 ep_reward:-56.0\r",
      "Episode#:0 ep_reward:-57.0\r",
      "Episode#:0 ep_reward:-58.0\r",
      "Episode#:0 ep_reward:-59.0\r",
      "Episode#:0 ep_reward:-60.0\r",
      "Episode#:0 ep_reward:-61.0\r",
      "Episode#:0 ep_reward:-62.0\r",
      "Episode#:0 ep_reward:-63.0\r",
      "Episode#:0 ep_reward:-64.0\r",
      "Episode#:0 ep_reward:-65.0\r",
      "Episode#:0 ep_reward:-66.0\r",
      "Episode#:0 ep_reward:-67.0\r",
      "Episode#:0 ep_reward:-68.0\r",
      "Episode#:0 ep_reward:-69.0\r",
      "Episode#:0 ep_reward:-70.0\r",
      "Episode#:0 ep_reward:-71.0\r",
      "Episode#:0 ep_reward:-72.0\r",
      "Episode#:0 ep_reward:-73.0\r",
      "Episode#:0 ep_reward:-74.0\r",
      "Episode#:0 ep_reward:-75.0\r",
      "Episode#:0 ep_reward:-76.0\r",
      "Episode#:0 ep_reward:-77.0\r",
      "Episode#:0 ep_reward:-78.0\r",
      "Episode#:0 ep_reward:-79.0\r",
      "Episode#:0 ep_reward:-80.0\r",
      "Episode#:0 ep_reward:-81.0\r",
      "Episode#:0 ep_reward:-82.0\r",
      "Episode#:0 ep_reward:-83.0\r",
      "Episode#:0 ep_reward:-84.0\r",
      "Episode#:0 ep_reward:-85.0\r",
      "Episode#:0 ep_reward:-86.0\r",
      "Episode#:0 ep_reward:-87.0\r",
      "Episode#:0 ep_reward:-88.0\r",
      "Episode#:0 ep_reward:-89.0\r",
      "Episode#:0 ep_reward:-90.0\r",
      "Episode#:0 ep_reward:-91.0\r",
      "Episode#:0 ep_reward:-92.0\r",
      "Episode#:0 ep_reward:-93.0\r",
      "Episode#:0 ep_reward:-94.0\r",
      "Episode#:0 ep_reward:-95.0\r",
      "Episode#:0 ep_reward:-96.0\r",
      "Episode#:0 ep_reward:-97.0\r",
      "Episode#:0 ep_reward:-98.0\r",
      "Episode#:0 ep_reward:-99.0\r",
      "Episode#:0 ep_reward:-100.0\r",
      "Episode#:0 ep_reward:-101.0\r",
      "Episode#:0 ep_reward:-102.0\r",
      "Episode#:0 ep_reward:-103.0\r",
      "Episode#:0 ep_reward:-104.0\r",
      "Episode#:0 ep_reward:-105.0\r",
      "Episode#:0 ep_reward:-106.0\r",
      "Episode#:0 ep_reward:-107.0\r",
      "Episode#:0 ep_reward:-108.0\r",
      "Episode#:0 ep_reward:-109.0\r",
      "Episode#:0 ep_reward:-110.0\r",
      "Episode#:0 ep_reward:-111.0\r",
      "Episode#:0 ep_reward:-112.0\r",
      "Episode#:0 ep_reward:-113.0\r",
      "Episode#:0 ep_reward:-114.0\r",
      "Episode#:0 ep_reward:-115.0\r",
      "Episode#:0 ep_reward:-116.0\r",
      "Episode#:0 ep_reward:-117.0\r",
      "Episode#:0 ep_reward:-118.0\r",
      "Episode#:0 ep_reward:-119.0\r",
      "Episode#:0 ep_reward:-120.0\r",
      "Episode#:0 ep_reward:-121.0\r",
      "Episode#:0 ep_reward:-122.0\r",
      "Episode#:0 ep_reward:-123.0\r",
      "Episode#:0 ep_reward:-124.0\r",
      "Episode#:0 ep_reward:-125.0\r",
      "Episode#:0 ep_reward:-126.0\r",
      "Episode#:0 ep_reward:-127.0\r",
      "Episode#:0 ep_reward:-128.0\r",
      "Episode#:0 ep_reward:-129.0\r",
      "Episode#:0 ep_reward:-130.0\r",
      "Episode#:0 ep_reward:-131.0\r",
      "Episode#:0 ep_reward:-132.0\r",
      "Episode#:0 ep_reward:-133.0\r",
      "Episode#:0 ep_reward:-134.0\r",
      "Episode#:0 ep_reward:-135.0\r",
      "Episode#:0 ep_reward:-136.0\r",
      "Episode#:0 ep_reward:-137.0\r",
      "Episode#:0 ep_reward:-138.0\r",
      "Episode#:0 ep_reward:-139.0\r",
      "Episode#:0 ep_reward:-140.0\r",
      "Episode#:0 ep_reward:-141.0\r",
      "Episode#:0 ep_reward:-142.0\r",
      "Episode#:0 ep_reward:-143.0\r",
      "Episode#:0 ep_reward:-144.0\r",
      "Episode#:0 ep_reward:-145.0\r",
      "Episode#:0 ep_reward:-146.0\r",
      "Episode#:0 ep_reward:-147.0\r",
      "Episode#:0 ep_reward:-148.0\r",
      "Episode#:0 ep_reward:-149.0\r",
      "Episode#:0 ep_reward:-150.0\r",
      "Episode#:0 ep_reward:-151.0\r",
      "Episode#:0 ep_reward:-152.0\r",
      "Episode#:0 ep_reward:-153.0\r",
      "Episode#:0 ep_reward:-154.0\r",
      "Episode#:0 ep_reward:-155.0\r",
      "Episode#:0 ep_reward:-156.0\r",
      "Episode#:0 ep_reward:-157.0\r",
      "Episode#:0 ep_reward:-158.0\r",
      "Episode#:0 ep_reward:-159.0\r",
      "Episode#:0 ep_reward:-160.0\r",
      "Episode#:0 ep_reward:-161.0\r",
      "Episode#:0 ep_reward:-162.0\r",
      "Episode#:0 ep_reward:-163.0\r",
      "Episode#:0 ep_reward:-164.0\r",
      "Episode#:0 ep_reward:-165.0\r",
      "Episode#:0 ep_reward:-166.0\r",
      "Episode#:0 ep_reward:-167.0\r",
      "Episode#:0 ep_reward:-168.0\r",
      "Episode#:0 ep_reward:-169.0\r",
      "Episode#:0 ep_reward:-170.0\r",
      "Episode#:0 ep_reward:-171.0\r",
      "Episode#:0 ep_reward:-172.0\r",
      "Episode#:0 ep_reward:-173.0\r",
      "Episode#:0 ep_reward:-174.0\r",
      "Episode#:0 ep_reward:-175.0\r",
      "Episode#:0 ep_reward:-176.0\r",
      "Episode#:0 ep_reward:-177.0\r",
      "Episode#:0 ep_reward:-178.0\r",
      "Episode#:0 ep_reward:-179.0\r",
      "Episode#:0 ep_reward:-180.0\r",
      "Episode#:0 ep_reward:-181.0\r",
      "Episode#:0 ep_reward:-182.0\r",
      "Episode#:0 ep_reward:-183.0\r",
      "Episode#:0 ep_reward:-184.0\r",
      "Episode#:0 ep_reward:-185.0\r",
      "Episode#:0 ep_reward:-186.0\r",
      "Episode#:0 ep_reward:-187.0\r",
      "Episode#:0 ep_reward:-188.0\r",
      "Episode#:0 ep_reward:-189.0\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#:0 ep_reward:-190.0\r",
      "Episode#:0 ep_reward:-191.0\r",
      "Episode#:0 ep_reward:-192.0\r",
      "Episode#:0 ep_reward:-193.0\r",
      "Episode#:0 ep_reward:-194.0\r",
      "Episode#:0 ep_reward:-195.0\r",
      "Episode#:0 ep_reward:-196.0\r",
      "Episode#:0 ep_reward:-197.0\r",
      "Episode#:0 ep_reward:-198.0\r",
      "Episode#:0 ep_reward:-199.0\r",
      "WARNING:tensorflow:Layer policy_net is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Episode#:0 ep_reward:-200.0\r",
      "Episode#:1 ep_reward:-1.0\r",
      "Episode#:1 ep_reward:-2.0\r",
      "Episode#:1 ep_reward:-3.0\r",
      "Episode#:1 ep_reward:-4.0\r",
      "Episode#:1 ep_reward:-5.0\r",
      "Episode#:1 ep_reward:-6.0\r",
      "Episode#:1 ep_reward:-7.0\r",
      "Episode#:1 ep_reward:-8.0\r",
      "Episode#:1 ep_reward:-9.0\r",
      "Episode#:1 ep_reward:-10.0\r",
      "Episode#:1 ep_reward:-11.0\r",
      "Episode#:1 ep_reward:-12.0\r",
      "Episode#:1 ep_reward:-13.0\r",
      "Episode#:1 ep_reward:-14.0\r",
      "Episode#:1 ep_reward:-15.0\r",
      "Episode#:1 ep_reward:-16.0\r",
      "Episode#:1 ep_reward:-17.0\r",
      "Episode#:1 ep_reward:-18.0\r",
      "Episode#:1 ep_reward:-19.0\r",
      "Episode#:1 ep_reward:-20.0\r",
      "Episode#:1 ep_reward:-21.0\r",
      "Episode#:1 ep_reward:-22.0\r",
      "Episode#:1 ep_reward:-23.0\r",
      "Episode#:1 ep_reward:-24.0\r",
      "Episode#:1 ep_reward:-25.0\r",
      "Episode#:1 ep_reward:-26.0\r",
      "Episode#:1 ep_reward:-27.0\r",
      "Episode#:1 ep_reward:-28.0\r",
      "Episode#:1 ep_reward:-29.0\r",
      "Episode#:1 ep_reward:-30.0\r",
      "Episode#:1 ep_reward:-31.0\r",
      "Episode#:1 ep_reward:-32.0\r",
      "Episode#:1 ep_reward:-33.0\r",
      "Episode#:1 ep_reward:-34.0\r",
      "Episode#:1 ep_reward:-35.0\r",
      "Episode#:1 ep_reward:-36.0\r",
      "Episode#:1 ep_reward:-37.0\r",
      "Episode#:1 ep_reward:-38.0\r",
      "Episode#:1 ep_reward:-39.0\r",
      "Episode#:1 ep_reward:-40.0\r",
      "Episode#:1 ep_reward:-41.0\r",
      "Episode#:1 ep_reward:-42.0\r",
      "Episode#:1 ep_reward:-43.0\r",
      "Episode#:1 ep_reward:-44.0\r",
      "Episode#:1 ep_reward:-45.0\r",
      "Episode#:1 ep_reward:-46.0\r",
      "Episode#:1 ep_reward:-47.0\r",
      "Episode#:1 ep_reward:-48.0\r",
      "Episode#:1 ep_reward:-49.0\r",
      "Episode#:1 ep_reward:-50.0\r",
      "Episode#:1 ep_reward:-51.0\r",
      "Episode#:1 ep_reward:-52.0\r",
      "Episode#:1 ep_reward:-53.0\r",
      "Episode#:1 ep_reward:-54.0\r",
      "Episode#:1 ep_reward:-55.0\r",
      "Episode#:1 ep_reward:-56.0\r",
      "Episode#:1 ep_reward:-57.0\r",
      "Episode#:1 ep_reward:-58.0\r",
      "Episode#:1 ep_reward:-59.0\r",
      "Episode#:1 ep_reward:-60.0\r",
      "Episode#:1 ep_reward:-61.0\r",
      "Episode#:1 ep_reward:-62.0\r",
      "Episode#:1 ep_reward:-63.0\r",
      "Episode#:1 ep_reward:-64.0\r",
      "Episode#:1 ep_reward:-65.0\r",
      "Episode#:1 ep_reward:-66.0\r",
      "Episode#:1 ep_reward:-67.0\r",
      "Episode#:1 ep_reward:-68.0\r",
      "Episode#:1 ep_reward:-69.0\r",
      "Episode#:1 ep_reward:-70.0\r",
      "Episode#:1 ep_reward:-71.0\r",
      "Episode#:1 ep_reward:-72.0\r",
      "Episode#:1 ep_reward:-73.0\r",
      "Episode#:1 ep_reward:-74.0\r",
      "Episode#:1 ep_reward:-75.0\r",
      "Episode#:1 ep_reward:-76.0\r",
      "Episode#:1 ep_reward:-77.0\r",
      "Episode#:1 ep_reward:-78.0\r",
      "Episode#:1 ep_reward:-79.0\r",
      "Episode#:1 ep_reward:-80.0\r",
      "Episode#:1 ep_reward:-81.0\r",
      "Episode#:1 ep_reward:-82.0\r",
      "Episode#:1 ep_reward:-83.0\r",
      "Episode#:1 ep_reward:-84.0\r",
      "Episode#:1 ep_reward:-85.0\r",
      "Episode#:1 ep_reward:-86.0\r",
      "Episode#:1 ep_reward:-87.0\r",
      "Episode#:1 ep_reward:-88.0\r",
      "Episode#:1 ep_reward:-89.0\r",
      "Episode#:1 ep_reward:-90.0\r",
      "Episode#:1 ep_reward:-91.0\r",
      "Episode#:1 ep_reward:-92.0\r",
      "Episode#:1 ep_reward:-93.0\r",
      "Episode#:1 ep_reward:-94.0\r",
      "Episode#:1 ep_reward:-95.0\r",
      "Episode#:1 ep_reward:-96.0\r",
      "Episode#:1 ep_reward:-97.0\r",
      "Episode#:1 ep_reward:-98.0\r",
      "Episode#:1 ep_reward:-99.0\r",
      "Episode#:1 ep_reward:-100.0\r",
      "Episode#:1 ep_reward:-101.0\r",
      "Episode#:1 ep_reward:-102.0\r",
      "Episode#:1 ep_reward:-103.0\r",
      "Episode#:1 ep_reward:-104.0\r",
      "Episode#:1 ep_reward:-105.0\r",
      "Episode#:1 ep_reward:-106.0\r",
      "Episode#:1 ep_reward:-107.0\r",
      "Episode#:1 ep_reward:-108.0\r",
      "Episode#:1 ep_reward:-109.0\r",
      "Episode#:1 ep_reward:-110.0\r",
      "Episode#:1 ep_reward:-111.0\r",
      "Episode#:1 ep_reward:-112.0\r",
      "Episode#:1 ep_reward:-113.0\r",
      "Episode#:1 ep_reward:-114.0\r",
      "Episode#:1 ep_reward:-115.0\r",
      "Episode#:1 ep_reward:-116.0\r",
      "Episode#:1 ep_reward:-117.0\r",
      "Episode#:1 ep_reward:-118.0\r",
      "Episode#:1 ep_reward:-119.0\r",
      "Episode#:1 ep_reward:-120.0\r",
      "Episode#:1 ep_reward:-121.0\r",
      "Episode#:1 ep_reward:-122.0\r",
      "Episode#:1 ep_reward:-123.0\r",
      "Episode#:1 ep_reward:-124.0\r",
      "Episode#:1 ep_reward:-125.0\r",
      "Episode#:1 ep_reward:-126.0\r",
      "Episode#:1 ep_reward:-127.0\r",
      "Episode#:1 ep_reward:-128.0\r",
      "Episode#:1 ep_reward:-129.0\r",
      "Episode#:1 ep_reward:-130.0\r",
      "Episode#:1 ep_reward:-131.0\r",
      "Episode#:1 ep_reward:-132.0\r",
      "Episode#:1 ep_reward:-133.0\r",
      "Episode#:1 ep_reward:-134.0\r",
      "Episode#:1 ep_reward:-135.0\r",
      "Episode#:1 ep_reward:-136.0\r",
      "Episode#:1 ep_reward:-137.0\r",
      "Episode#:1 ep_reward:-138.0\r",
      "Episode#:1 ep_reward:-139.0\r",
      "Episode#:1 ep_reward:-140.0\r",
      "Episode#:1 ep_reward:-141.0\r",
      "Episode#:1 ep_reward:-142.0\r",
      "Episode#:1 ep_reward:-143.0\r",
      "Episode#:1 ep_reward:-144.0\r",
      "Episode#:1 ep_reward:-145.0\r",
      "Episode#:1 ep_reward:-146.0\r",
      "Episode#:1 ep_reward:-147.0\r",
      "Episode#:1 ep_reward:-148.0\r",
      "Episode#:1 ep_reward:-149.0\r",
      "Episode#:1 ep_reward:-150.0\r",
      "Episode#:1 ep_reward:-151.0\r",
      "Episode#:1 ep_reward:-152.0\r",
      "Episode#:1 ep_reward:-153.0\r",
      "Episode#:1 ep_reward:-154.0\r",
      "Episode#:1 ep_reward:-155.0\r",
      "Episode#:1 ep_reward:-156.0\r",
      "Episode#:1 ep_reward:-157.0\r",
      "Episode#:1 ep_reward:-158.0\r",
      "Episode#:1 ep_reward:-159.0\r",
      "Episode#:1 ep_reward:-160.0\r",
      "Episode#:1 ep_reward:-161.0\r",
      "Episode#:1 ep_reward:-162.0\r",
      "Episode#:1 ep_reward:-163.0\r",
      "Episode#:1 ep_reward:-164.0\r",
      "Episode#:1 ep_reward:-165.0\r",
      "Episode#:1 ep_reward:-166.0\r",
      "Episode#:1 ep_reward:-167.0\r",
      "Episode#:1 ep_reward:-168.0\r",
      "Episode#:1 ep_reward:-169.0\r",
      "Episode#:1 ep_reward:-170.0\r",
      "Episode#:1 ep_reward:-171.0\r",
      "Episode#:1 ep_reward:-172.0\r",
      "Episode#:1 ep_reward:-173.0\r",
      "Episode#:1 ep_reward:-174.0\r",
      "Episode#:1 ep_reward:-175.0\r",
      "Episode#:1 ep_reward:-176.0\r",
      "Episode#:1 ep_reward:-177.0\r",
      "Episode#:1 ep_reward:-178.0\r",
      "Episode#:1 ep_reward:-179.0\r",
      "Episode#:1 ep_reward:-180.0\r",
      "Episode#:1 ep_reward:-181.0\r",
      "Episode#:1 ep_reward:-182.0\r",
      "Episode#:1 ep_reward:-183.0\r",
      "Episode#:1 ep_reward:-184.0\r",
      "Episode#:1 ep_reward:-185.0\r",
      "Episode#:1 ep_reward:-186.0\r",
      "Episode#:1 ep_reward:-187.0\r",
      "Episode#:1 ep_reward:-188.0\r",
      "Episode#:1 ep_reward:-189.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode#:1 ep_reward:-190.0\r",
      "Episode#:1 ep_reward:-191.0\r",
      "Episode#:1 ep_reward:-192.0\r",
      "Episode#:1 ep_reward:-193.0\r",
      "Episode#:1 ep_reward:-194.0\r",
      "Episode#:1 ep_reward:-195.0\r",
      "Episode#:1 ep_reward:-196.0\r",
      "Episode#:1 ep_reward:-197.0\r",
      "Episode#:1 ep_reward:-198.0\r",
      "Episode#:1 ep_reward:-199.0\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Episode#:1 ep_reward:-200.0\r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    episodes = 2  #  Increase number of episodes to train\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    # Set render=True to visualize Agent's actions in the env\n",
    "    train(agent, env, episodes, render=False)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "tfrl-cookbook",
   "language": "python",
   "name": "tfrl-cookbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
