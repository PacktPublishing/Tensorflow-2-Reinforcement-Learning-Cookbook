{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximal Policy Optimizatin (PPO) agent training script\n",
    "Chapter 3, TensorFlow 2 Reinforcement Learning Cookbook | Praveen Palanisamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--logdir'], dest='logdir', nargs=None, const=None, default='logs', type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog=\"TFRL-Cookbook-Ch3-PPO\")\n",
    "parser.add_argument(\"--env\", default=\"Pendulum-v0\")\n",
    "parser.add_argument(\"--update-freq\", type=int, default=5)\n",
    "parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "parser.add_argument(\"--actor-lr\", type=float, default=0.0005)\n",
    "parser.add_argument(\"--critic-lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--clip-ratio\", type=float, default=0.1)\n",
    "parser.add_argument(\"--gae-lambda\", type=float, default=0.95)\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "parser.add_argument(\"--logdir\", default=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training logs to:logs/TFRL-Cookbook-Ch3-PPO/Pendulum-v0/20221216-041316\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args([])\n",
    "logdir = os.path.join(\n",
    "    args.logdir, parser.prog, args.env, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    ")\n",
    "print(f\"Saving training logs to:{logdir}\")\n",
    "writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.std_bound = std_bound\n",
    "        self.model = self.nn_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(args.actor_lr)\n",
    "\n",
    "    def nn_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        dense_1 = Dense(32, activation=\"relu\")(state_input)\n",
    "        dense_2 = Dense(32, activation=\"relu\")(dense_1)\n",
    "        out_mu = Dense(self.action_dim, activation=\"tanh\")(dense_2)\n",
    "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
    "        std_output = Dense(self.action_dim, activation=\"softplus\")(dense_2)\n",
    "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        mu, std = self.model.predict(state)\n",
    "        action = np.random.normal(mu[0], std[0], size=self.action_dim)\n",
    "        action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "        log_policy = self.log_pdf(mu, std, action)\n",
    "\n",
    "        return log_policy, action\n",
    "\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n",
    "            var * 2 * np.pi\n",
    "        )\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, log_old_policy, log_new_policy, actions, gaes):\n",
    "        ratio = tf.exp(log_new_policy - tf.stop_gradient(log_old_policy))\n",
    "        gaes = tf.stop_gradient(gaes)\n",
    "        clipped_ratio = tf.clip_by_value(\n",
    "            ratio, 1.0 - args.clip_ratio, 1.0 + args.clip_ratio\n",
    "        )\n",
    "        surrogate = -tf.minimum(ratio * gaes, clipped_ratio * gaes)\n",
    "        return tf.reduce_mean(surrogate)\n",
    "\n",
    "    def train(self, log_old_policy, states, actions, gaes):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.model(states, training=True)\n",
    "            log_new_policy = self.log_pdf(mu, std, actions)\n",
    "            loss = self.compute_loss(log_old_policy, log_new_policy, actions, gaes)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.nn_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(args.critic_lr)\n",
    "\n",
    "    def nn_model(self):\n",
    "        return tf.keras.Sequential(\n",
    "            [\n",
    "                Input((self.state_dim,)),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dense(16, activation=\"relu\"),\n",
    "                Dense(1, activation=\"linear\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            # assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.actor_opt = tf.keras.optimizers.Adam(args.actor_lr)\n",
    "        self.critic_opt = tf.keras.optimizers.Adam(args.critic_lr)\n",
    "        self.actor = Actor(\n",
    "            self.state_dim, self.action_dim, self.action_bound, self.std_bound\n",
    "        )\n",
    "        self.critic = Critic(self.state_dim)\n",
    "\n",
    "    def gae_target(self, rewards, v_values, next_v_value, done):\n",
    "        n_step_targets = np.zeros_like(rewards)\n",
    "        gae = np.zeros_like(rewards)\n",
    "        gae_cumulative = 0\n",
    "        forward_val = 0\n",
    "\n",
    "        if not done:\n",
    "            forward_val = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            delta = rewards[k] + args.gamma * forward_val - v_values[k]\n",
    "            gae_cumulative = args.gamma * args.gae_lambda * gae_cumulative + delta\n",
    "            gae[k] = gae_cumulative\n",
    "            forward_val = v_values[k]\n",
    "            n_step_targets[k] = gae[k] + v_values[k]\n",
    "        return gae, n_step_targets\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        with writer.as_default():\n",
    "            for ep in range(max_episodes):\n",
    "                state_batch = []\n",
    "                action_batch = []\n",
    "                reward_batch = []\n",
    "                old_policy_batch = []\n",
    "\n",
    "                episode_reward, done = 0, False\n",
    "\n",
    "                state = self.env.reset()\n",
    "\n",
    "                while not done:\n",
    "                    # self.env.render()\n",
    "                    log_old_policy, action = self.actor.get_action(state)\n",
    "\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                    state = np.reshape(state, [1, self.state_dim])\n",
    "                    action = np.reshape(action, [1, 1])\n",
    "                    next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                    reward = np.reshape(reward, [1, 1])\n",
    "                    log_old_policy = np.reshape(log_old_policy, [1, 1])\n",
    "\n",
    "                    state_batch.append(state)\n",
    "                    action_batch.append(action)\n",
    "                    reward_batch.append((reward + 8) / 8)\n",
    "                    old_policy_batch.append(log_old_policy)\n",
    "\n",
    "                    if len(state_batch) >= args.update_freq or done:\n",
    "                        states = np.array([state.squeeze() for state in state_batch])\n",
    "                        actions = np.array(\n",
    "                            [action.squeeze() for action in action_batch]\n",
    "                        )\n",
    "                        rewards = np.array(\n",
    "                            [reward.squeeze() for reward in reward_batch]\n",
    "                        )\n",
    "                        old_policies = np.array(\n",
    "                            [old_pi.squeeze() for old_pi in old_policy_batch]\n",
    "                        )\n",
    "\n",
    "                        v_values = self.critic.model.predict(states)\n",
    "                        next_v_value = self.critic.model.predict(next_state)\n",
    "\n",
    "                        gaes, td_targets = self.gae_target(\n",
    "                            rewards, v_values, next_v_value, done\n",
    "                        )\n",
    "                        actor_losses, critic_losses = [], []\n",
    "                        for epoch in range(args.epochs):\n",
    "                            actor_loss = self.actor.train(\n",
    "                                old_policies, states, actions, gaes\n",
    "                            )\n",
    "                            actor_losses.append(actor_loss)\n",
    "                            critic_loss = self.critic.train(states, td_targets)\n",
    "                            critic_losses.append(critic_loss)\n",
    "                        # Plot mean actor & critic losses on every update\n",
    "                        tf.summary.scalar(\"actor_loss\", np.mean(actor_losses), step=ep)\n",
    "                        tf.summary.scalar(\n",
    "                            \"critic_loss\", np.mean(critic_losses), step=ep\n",
    "                        )\n",
    "\n",
    "                        state_batch = []\n",
    "                        action_batch = []\n",
    "                        reward_batch = []\n",
    "                        old_policy_batch = []\n",
    "\n",
    "                    episode_reward += reward[0][0]\n",
    "                    state = next_state[0]\n",
    "\n",
    "                print(f\"Episode#{ep} Reward:{episode_reward}\")\n",
    "                tf.summary.scalar(\"episode_reward\", episode_reward, step=ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#0 Reward:-1028.2436740177866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#1 Reward:-973.3971378764863\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_name = \"Pendulum-v0\"\n",
    "    env = gym.make(env_name)\n",
    "    agent = Agent(env)\n",
    "    agent.train(max_episodes=2)  # Increase max_episodes value"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "tfrl-cookbook",
   "language": "python",
   "name": "tfrl-cookbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
