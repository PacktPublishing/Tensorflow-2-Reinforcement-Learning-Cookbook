{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Deterministic Policy Gradients (DDPG) agent training script\n",
    "Chapter 3, TensorFlow 2 Reinforcement Learning Cookbook | Praveen Palanisamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--logdir'], dest='logdir', nargs=None, const=None, default='logs', type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog=\"TFRL-Cookbook-Ch3-DDPG\")\n",
    "parser.add_argument(\"--env\", default=\"Pendulum-v0\")\n",
    "parser.add_argument(\"--actor_lr\", type=float, default=0.0005)\n",
    "parser.add_argument(\"--critic_lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--tau\", type=float, default=0.05)\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "parser.add_argument(\"--train_start\", type=int, default=2000)\n",
    "parser.add_argument(\"--logdir\", default=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training logs to:logs/TFRL-Cookbook-Ch3-DDPG/Pendulum-v0/20210519-060928\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args([])\n",
    "logdir = os.path.join(\n",
    "    args.logdir, parser.prog, args.env, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    ")\n",
    "print(f\"Saving training logs to:{logdir}\")\n",
    "writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "\n",
    "    def sample(self):\n",
    "        sample = random.sample(self.buffer, args.batch_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n",
    "        states = np.array(states).reshape(args.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(args.batch_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.model = self.nn_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(args.actor_lr)\n",
    "\n",
    "    def nn_model(self):\n",
    "        return tf.keras.Sequential(\n",
    "            [\n",
    "                Input((self.state_dim,)),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dense(self.action_dim, activation=\"tanh\"),\n",
    "                Lambda(lambda x: x * self.action_bound),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def train(self, states, q_grads):\n",
    "        with tf.GradientTape() as tape:\n",
    "            grads = tape.gradient(\n",
    "                self.model(states), self.model.trainable_variables, -q_grads\n",
    "            )\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        return self.model.predict(state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self.nn_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(args.critic_lr)\n",
    "\n",
    "    def nn_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        s1 = Dense(64, activation=\"relu\")(state_input)\n",
    "        s2 = Dense(32, activation=\"relu\")(s1)\n",
    "        action_input = Input((self.action_dim,))\n",
    "        a1 = Dense(32, activation=\"relu\")(action_input)\n",
    "        c1 = concatenate([s2, a1], axis=-1)\n",
    "        c2 = Dense(16, activation=\"relu\")(c1)\n",
    "        output = Dense(1, activation=\"linear\")(c2)\n",
    "        return tf.keras.Model([state_input, action_input], output)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.model.predict(inputs)\n",
    "\n",
    "    def q_gradients(self, states, actions):\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(actions)\n",
    "            q_values = self.model([states, actions])\n",
    "            q_values = tf.squeeze(q_values)\n",
    "        return tape.gradient(q_values, actions)\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, actions, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model([states, actions], training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim)\n",
    "\n",
    "        self.target_actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.target_critic = Critic(self.state_dim, self.action_dim)\n",
    "\n",
    "        actor_weights = self.actor.model.get_weights()\n",
    "        critic_weights = self.critic.model.get_weights()\n",
    "        self.target_actor.model.set_weights(actor_weights)\n",
    "        self.target_critic.model.set_weights(critic_weights)\n",
    "\n",
    "    def update_target(self):\n",
    "        actor_weights = self.actor.model.get_weights()\n",
    "        t_actor_weights = self.target_actor.model.get_weights()\n",
    "        critic_weights = self.critic.model.get_weights()\n",
    "        t_critic_weights = self.target_critic.model.get_weights()\n",
    "\n",
    "        for i in range(len(actor_weights)):\n",
    "            t_actor_weights[i] = (\n",
    "                args.tau * actor_weights[i] + (1 - args.tau) * t_actor_weights[i]\n",
    "            )\n",
    "\n",
    "        for i in range(len(critic_weights)):\n",
    "            t_critic_weights[i] = (\n",
    "                args.tau * critic_weights[i] + (1 - args.tau) * t_critic_weights[i]\n",
    "            )\n",
    "\n",
    "        self.target_actor.model.set_weights(t_actor_weights)\n",
    "        self.target_critic.model.set_weights(t_critic_weights)\n",
    "\n",
    "    def get_td_target(self, rewards, q_values, dones):\n",
    "        targets = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                targets[i] = rewards[i]\n",
    "            else:\n",
    "                targets[i] = args.gamma * q_values[i]\n",
    "        return targets\n",
    "\n",
    "    def add_ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n",
    "        return (\n",
    "            x + rho * (mu - x) * dt + sigma * np.sqrt(dt) * np.random.normal(size=dim)\n",
    "        )\n",
    "\n",
    "    def replay_experience(self):\n",
    "        for _ in range(10):\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "            target_q_values = self.target_critic.predict(\n",
    "                [next_states, self.target_actor.predict(next_states)]\n",
    "            )\n",
    "            td_targets = self.get_td_target(rewards, target_q_values, dones)\n",
    "\n",
    "            self.critic.train(states, actions, td_targets)\n",
    "\n",
    "            s_actions = self.actor.predict(states)\n",
    "            s_grads = self.critic.q_gradients(states, s_actions)\n",
    "            grads = np.array(s_grads).reshape((-1, self.action_dim))\n",
    "            self.actor.train(states, grads)\n",
    "            self.update_target()\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        with writer.as_default():\n",
    "            for ep in range(max_episodes):\n",
    "                episode_reward, done = 0, False\n",
    "\n",
    "                state = self.env.reset()\n",
    "                bg_noise = np.zeros(self.action_dim)\n",
    "                while not done:\n",
    "                    # self.env.render()\n",
    "                    action = self.actor.get_action(state)\n",
    "                    noise = self.add_ou_noise(bg_noise, dim=self.action_dim)\n",
    "                    action = np.clip(\n",
    "                        action + noise, -self.action_bound, self.action_bound\n",
    "                    )\n",
    "\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    self.buffer.store(state, action, (reward + 8) / 8, next_state, done)\n",
    "                    bg_noise = noise\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                if (\n",
    "                    self.buffer.size() >= args.batch_size\n",
    "                    and self.buffer.size() >= args.train_start\n",
    "                ):\n",
    "                    self.replay_experience()\n",
    "                print(f\"Episode#{ep} Reward:{episode_reward}\")\n",
    "                tf.summary.scalar(\"episode_reward\", episode_reward, step=ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#0 Reward:-1432.6733864517835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#1 Reward:-1200.3101983958761\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_name = \"Pendulum-v0\"\n",
    "    env = gym.make(env_name)\n",
    "    agent = Agent(env)\n",
    "    agent.train(max_episodes=2)  # Increase max_episodes value"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "tfrl-cookbook",
   "language": "python",
   "name": "tfrl-cookbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
